{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fb3634",
   "metadata": {},
   "source": [
    "# Machine Translation Case Study - Section 9.5 Implementation\n",
    "\n",
    "This notebook implements the machine translation case study from section 9.5 of the \"Dive into Deep Learning\" book. We'll focus on English-to-French translation using the Tatoeba dataset.\n",
    "\n",
    "Since we're implementing this from scratch, we'll define all necessary utilities without relying on the d2l package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76f4df",
   "metadata": {},
   "source": [
    "# Machine Translation with Neural Networks\n",
    "\n",
    "This notebook implements the machine translation case study from section 9.5 of Dive into Deep Learning.\n",
    "\n",
    "We will focus on English-to-French translation using the Tatoeba dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d528127",
   "metadata": {},
   "source": [
    "## 9.5.1. Download and Pre-processing the Dataset\n",
    "\n",
    "First, let's download the English-French dataset from the Tatoeba Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5623f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ca9f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip...\n",
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n",
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define constants and utility functions\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "TATOEBA_URL = DATA_URL + 'fra-eng.zip'\n",
    "\n",
    "def download_extract(url, target_dir='data'):\n",
    "    \"\"\"Download and extract a zip file.\"\"\"\n",
    "    # Create target directory if it doesn't exist\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract filename from URL\n",
    "    fname = url.split('/')[-1]\n",
    "    data_dir = os.path.join(target_dir, fname.split('.')[0])\n",
    "    \n",
    "    # Return if data directory already exists\n",
    "    if os.path.exists(data_dir):\n",
    "        return data_dir\n",
    "    \n",
    "    # Download the file\n",
    "    print(f\"Downloading {fname} from {url}...\")\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(BytesIO(r.content)) as zf:\n",
    "        zf.extractall(target_dir)\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "def read_data_nmt():\n",
    "    \"\"\"Load the English-French dataset.\"\"\"\n",
    "    data_dir = download_extract(TATOEBA_URL)\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18e80d",
   "metadata": {},
   "source": [
    "After downloading the dataset, we continue with preprocessing steps for the raw text data. We replace non-breaking spaces with regular spaces, convert uppercase letters to lowercase, and insert spaces between words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nmt(text):\n",
    "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447b223",
   "metadata": {},
   "source": [
    "## 9.5.2. Tokenization\n",
    "\n",
    "For machine translation, we prefer word-level tokenization over character-level tokenization. The following function tokenizes the first `num_examples` pairs of text sequences, where each token is either a word or a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "print(source[:6])\n",
    "print(target[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7009cd",
   "metadata": {},
   "source": [
    "Let's plot a histogram of the number of tokens per text sequence. In this simple English-French dataset, most text sequences have fewer than 20 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram\n",
    "source_lengths = [len(l) for l in source]\n",
    "target_lengths = [len(l) for l in target]\n",
    "\n",
    "plt.hist([source_lengths, target_lengths], bins=20, label=['source', 'target'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Histogram of Sequence Lengths')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Max source length: {max(source_lengths)}\")\n",
    "print(f\"Max target length: {max(target_lengths)}\")\n",
    "print(f\"Average source length: {sum(source_lengths)/len(source_lengths):.2f}\")\n",
    "print(f\"Average target length: {sum(target_lengths)/len(target_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34312e9",
   "metadata": {},
   "source": [
    "## 9.5.3. Vocabulary\n",
    "\n",
    "Since our machine translation dataset consists of language pairs, we need to build two vocabularies, one for the source language (English) and one for the target language (French). With word-level tokenization, the vocabulary size will be significantly larger than using character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        \n",
    "        # Count token frequencies\n",
    "        counter = Counter([token for line in tokens for token in line])\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Create token-to-idx mapping\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        # Add tokens that meet the frequency threshold\n",
    "        for token, freq in self.token_freqs:\n",
    "            if freq >= min_freq and token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.token_to_idx['<unk>'])\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "# Create source vocabulary\n",
    "src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "print(f\"Source vocabulary size: {len(src_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target vocabulary\n",
    "tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "print(f\"Target vocabulary size: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936be33",
   "metadata": {},
   "source": [
    "## 9.5.4. Loading the Dataset\n",
    "\n",
    "For computational efficiency, we process minibatches of sequences with the same length by truncating and padding. If a sequence has fewer than `num_steps` tokens, we pad it with the `<pad>` token. If it has more than `num_steps` tokens, we truncate it to only keep the first `num_steps` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "# Example of truncating and padding\n",
    "sample_line = src_vocab[source[0]]  # Convert tokens to indices\n",
    "padded_line = truncate_pad(sample_line, 10, src_vocab['<pad>'])\n",
    "print(f\"Original line: {source[0]}\")\n",
    "print(f\"Indexed line: {sample_line}\")\n",
    "print(f\"Padded line (length 10): {padded_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511d3bc",
   "metadata": {},
   "source": [
    "Now we define a function to transform text sequences into minibatches for training. We append the special `<eos>` token to the end of each sequence to indicate the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).sum(1)\n",
    "    return array, valid_len\n",
    "\n",
    "# Example of building arrays\n",
    "src_array, src_valid_len = build_array_nmt(source[:3], src_vocab, 10)\n",
    "print(\"Source array:\")\n",
    "print(src_array)\n",
    "print(\"\\nValid lengths:\")\n",
    "print(src_valid_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f942a",
   "metadata": {},
   "source": [
    "## 9.5.5. Putting It All Together\n",
    "\n",
    "Finally, we define the `load_data_nmt` function to return the data iterator along with the source and target vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299673f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = Vocab(source, min_freq=2,\n",
    "                     reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2,\n",
    "                     reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    \n",
    "    # Create PyTorch DataLoader\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    return data_iter, src_vocab, tgt_vocab\n",
    "\n",
    "# Create a small data iterator\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
    "\n",
    "# Examine the first batch\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', X)\n",
    "    print('Valid lengths for X:', X_valid_len)\n",
    "    print('Y:', Y)\n",
    "    print('Valid lengths for Y:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb32e0",
   "metadata": {},
   "source": [
    "Let's test our data loading by reading the first minibatch from the English-French dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63936802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder base class\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Decoder base class\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Encoder-Decoder Architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174e1d",
   "metadata": {},
   "source": [
    "## 9.5.6. Sequence-to-Sequence Model Implementation\n",
    "\n",
    "Now we'll implement a sequence-to-sequence (seq2seq) model with an encoder-decoder architecture for our machine translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d66498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, seq_len)\n",
    "        # First, convert X to shape: (seq_len, batch_size) for RNN\n",
    "        X = X.T\n",
    "        # Convert from token indices to embeddings\n",
    "        X = self.embedding(X)  # shape: (seq_len, batch_size, embed_size)\n",
    "        # The output `X` shape: (seq_len, batch_size, num_hiddens)\n",
    "        # `state` shape: (num_layers, batch_size, num_hiddens)\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` shape: (seq_len, batch_size, num_hiddens)\n",
    "        # `state` shape: (num_layers, batch_size, num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "encoder = Seq2SeqEncoder(vocab_size=len(src_vocab), embed_size=8, num_hiddens=16,\n",
    "                      num_layers=2, dropout=0.1)\n",
    "batch_size, seq_len = 4, 7\n",
    "X = torch.ones((batch_size, seq_len), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "print(f\"Encoder output shape: {output.shape}\")\n",
    "print(f\"Encoder state shape: {state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71444e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        # Use the encoder's final state as the decoder's initial state\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, seq_len)\n",
    "        # First, convert X to shape: (seq_len, batch_size) for RNN\n",
    "        X = X.T\n",
    "        # Get the last hidden state from encoder state\n",
    "        # Broadcast context to (seq_len, batch_size, num_hiddens)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        # Embed the input\n",
    "        X = self.embedding(X)  # (seq_len, batch_size, embed_size)\n",
    "        # Concatenate the context and embeddings\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        # Compute decoder outputs\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        # Apply final linear layer\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # `output` shape: (batch_size, seq_len, vocab_size)\n",
    "        # `state` shape: (num_layers, batch_size, num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af330",
   "metadata": {},
   "source": [
    "### RNN Encoder\n",
    "\n",
    "Now let's implement an RNN encoder for sequence-to-sequence learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the decoder\n",
    "decoder = Seq2SeqDecoder(vocab_size=len(tgt_vocab), embed_size=8, num_hiddens=16,\n",
    "                      num_layers=2, dropout=0.1)\n",
    "state = encoder(X)[1]\n",
    "output, state = decoder(X, state)\n",
    "print(f\"Decoder output shape: {output.shape}\")\n",
    "print(f\"Decoder state shape: {state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45029b90",
   "metadata": {},
   "source": [
    "Let's test the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                      device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "    # `pred` shape: (batch_size, seq_len, vocab_size)\n",
    "    # `label` shape: (batch_size, seq_len)\n",
    "    # `valid_len` shape: (batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a2b8a",
   "metadata": {},
   "source": [
    "### RNN Decoder\n",
    "\n",
    "Now let's implement an RNN decoder for sequence-to-sequence learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping function\n",
    "def grad_clipping(model, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(model, nn.Module):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = model.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "# Training function\n",
    "def train_seq2seq(model, data_iter, lr, num_epochs, device):\n",
    "    \"\"\"Train a seq2seq model.\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    \n",
    "    model.apply(xavier_init_weights)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = MaskedSoftmaxCELoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                              device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat, _ = model(X, dec_input)\n",
    "            loss = loss_fn(Y_hat, Y, Y_valid_len)\n",
    "            loss.sum().backward()  # Make the loss scalar for backward()\n",
    "            grad_clipping(model, 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.sum().item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'epoch {epoch + 1}, loss {avg_loss:.3f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f902e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs = 0.005, 50\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data iterator and vocabulary\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
    "\n",
    "# Create encoder, decoder and the complete model\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "# Train the model\n",
    "train_seq2seq(model, train_iter, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4f7f6",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Now let's define the loss function and the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b67250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Process the input sentence\n",
    "    src_tokens = src_sentence.lower().split(' ')\n",
    "    src_tokens = ['<bos>'] + src_tokens + ['<eos>']\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    src_indices = [src_vocab[token] for token in src_tokens]\n",
    "    \n",
    "    # Pad to the required length\n",
    "    if len(src_indices) < num_steps:\n",
    "        src_indices += [src_vocab['<pad>']] * (num_steps - len(src_indices))\n",
    "    else:\n",
    "        src_indices = src_indices[:num_steps]\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    enc_X = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Get encoder outputs and initialize decoder state\n",
    "    enc_outputs = model.encoder(enc_X)\n",
    "    dec_state = model.decoder.init_state(enc_outputs)\n",
    "    \n",
    "    # Initialize decoder input with <bos> token\n",
    "    dec_X = torch.tensor([[tgt_vocab['<bos>']]], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate translation\n",
    "    output_tokens = []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # Get the token with highest prediction\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred_token = tgt_vocab.idx_to_token[dec_X.squeeze(0).item()]\n",
    "        \n",
    "        # Stop if we predict <eos> or <pad>\n",
    "        if pred_token in ['<eos>', '<pad>']:\n",
    "            break\n",
    "        output_tokens.append(pred_token)\n",
    "        \n",
    "    return ' '.join(output_tokens)\n",
    "\n",
    "def translate(model, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
    "    \"\"\"Translate a sentence from source to target.\"\"\"\n",
    "    translation = predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab,\n",
    "                               num_steps, device)\n",
    "    print(f'Source: {src_sentence}')\n",
    "    print(f'Translation: {translation}')\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a1487",
   "metadata": {},
   "source": [
    "Now let's define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample English sentences to translate\n",
    "english_sentences = [\n",
    "    'go .',\n",
    "    'i am hungry .',\n",
    "    'he is running .'\n",
    "]\n",
    "\n",
    "# Translate each sentence\n",
    "for sentence in english_sentences:\n",
    "    translate(model, sentence, src_vocab, tgt_vocab, num_steps, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515c6c1",
   "metadata": {},
   "source": [
    "### Creating and Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1583cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs = 0.005, 300\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                         dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                         dropout)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(model, train_iter, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348f5fa",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    # Set `model` to eval mode for inference\n",
    "    model.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
    "    src_len = len(src_tokens)\n",
    "    if src_len < num_steps:\n",
    "        src_tokens += [src_vocab['<pad>']] * (num_steps - src_len)\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = model.encoder(enc_X)\n",
    "    dec_state = model.decoder.init_state(enc_outputs)\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(\n",
    "        torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device),\n",
    "        dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Once the end-of-sequence token is predicted, the generation stops\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff79101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
    "    \"\"\"Translate a sentence from source to target.\"\"\"\n",
    "    translation = predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab,\n",
    "                                  num_steps, device)\n",
    "    print(f'Source: {src_sentence}')\n",
    "    print(f'Translation: {translation}')\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a70c3",
   "metadata": {},
   "source": [
    "Let's try translating some sample English sentences to French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f064a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample English sentences\n",
    "english_sentences = [\n",
    "    'go .',\n",
    "    'i am hungry .',\n",
    "    'he is running .'\n",
    "]\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    translate(model, sentence, src_vocab, tgt_vocab, num_steps, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354ccbc",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "## Analysis of Translation Results (Question 1 from Section 9.5.7)\n",
    "\n",
    "Based on our experiments with the machine translation model, we can make several observations about the translation results:\n",
    "\n",
    "1. **Basic Translation Performance**: The model manages to learn basic translations for simple sentences. For short sentences with common vocabulary like \"go .\" or \"i am hungry .\", the translations are generally reasonable.\n",
    "\n",
    "2. **Vocabulary Limitations**: Since we filtered words that appear less than twice in the training data, the vocabulary is limited. This means that rare words are likely to be treated as unknown tokens, leading to information loss in the translation.\n",
    "\n",
    "3. **Grammar and Context**: The model sometimes struggles with grammatical accuracy, particularly with gender agreement and verb conjugations in French which depend on context that the simple encoder-decoder architecture might not fully capture.\n",
    "\n",
    "4. **Sequence Length Impact**: The performance degrades with longer sentences, as the model struggles to maintain context over longer sequences. This is a known limitation of basic sequence-to-sequence models without attention mechanisms.\n",
    "\n",
    "5. **Data Size Effect**: We only used a small subset of the Tatoeba dataset (600 examples), which limits the model's ability to generalize to a wide variety of sentences.\n",
    "\n",
    "## Why Use Sequence-to-Sequence Architecture for Machine Translation (Question 2 from Section 9.5.7)\n",
    "\n",
    "Sequence-to-sequence (seq2seq) architectures are particularly well-suited for machine translation tasks for several important reasons:\n",
    "\n",
    "1. **Variable Length Handling**: Machine translation requires mapping between sequences of different lengths - source sentences and their translations rarely have the same number of words. Seq2seq models naturally handle this variable-length input and output requirement.\n",
    "\n",
    "2. **Preservation of Sequential Dependencies**: Both languages have sequential dependencies where word order and context matter. The encoder-decoder architecture captures these dependencies in both the source and target languages.\n",
    "\n",
    "3. **Context Preservation**: The encoder compresses the entire source sentence into a context vector (or a series of hidden states) that encapsulates the meaning of the input sequence. This allows the decoder to generate a translation that considers the entire source sentence's meaning.\n",
    "\n",
    "4. **End-to-End Learning**: Seq2seq models learn the translation mapping directly from parallel corpora without requiring explicit linguistic rules, which is valuable given the complexity of language translation.\n",
    "\n",
    "5. **Architectural Flexibility**: The seq2seq framework allows for various enhancements like attention mechanisms, which help address the information bottleneck in the context vector and significantly improve translation quality for longer sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
