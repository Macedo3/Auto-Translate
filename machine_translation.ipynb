{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fb3634",
   "metadata": {},
   "source": [
    "# Estudo de Caso de Tradução Automática - Implementação da Seção 9.5\n",
    "\n",
    "Este notebook implementa o estudo de caso de tradução automática da seção 9.5 do livro \"Dive into Deep Learning\". Vamos focar na tradução de Inglês para Francês utilizando o conjunto de dados Tatoeba.\n",
    "\n",
    "Como estamos implementando isso do zero, vamos definir todas as utilidades necessárias sem depender do pacote d2l."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76f4df",
   "metadata": {},
   "source": [
    "# Tradução Automática com Redes Neurais\n",
    "\n",
    "Este notebook implementa o estudo de caso de tradução automática da seção 9.5 do livro Dive into Deep Learning.\n",
    "\n",
    "Vamos focar na tradução de Inglês para Francês utilizando o conjunto de dados Tatoeba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d528127",
   "metadata": {},
   "source": [
    "## 9.5.1. Download e Pré-processamento do Conjunto de Dados\n",
    "\n",
    "Primeiro, vamos baixar o conjunto de dados Inglês-Francês do Projeto Tatoeba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5623f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca9f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip...\n",
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n",
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir constantes e funções utilitárias\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "TATOEBA_URL = DATA_URL + 'fra-eng.zip'\n",
    "\n",
    "def download_extract(url, target_dir='data'):\n",
    "    \"\"\"Baixa e extrai um arquivo zip.\"\"\"\n",
    "    # Cria o diretório de destino se não existir\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Extrai o nome do arquivo da URL\n",
    "    fname = url.split('/')[-1]\n",
    "    data_dir = os.path.join(target_dir, fname.split('.')[0])\n",
    "    \n",
    "    # Retorna se o diretório de dados já existir\n",
    "    if os.path.exists(data_dir):\n",
    "        return data_dir\n",
    "    \n",
    "    # Baixa o arquivo\n",
    "    print(f\"Baixando {fname} de {url}...\")\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Extrai o arquivo zip\n",
    "    with zipfile.ZipFile(BytesIO(r.content)) as zf:\n",
    "        zf.extractall(target_dir)\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "def read_data_nmt():\n",
    "    \"\"\"Carrega o conjunto de dados Inglês-Francês.\"\"\"\n",
    "    data_dir = download_extract(TATOEBA_URL)\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18e80d",
   "metadata": {},
   "source": [
    "Após baixar o conjunto de dados, continuamos com as etapas de pré-processamento para os dados de texto bruto. Substituímos espaços não-quebráveis por espaços regulares, convertemos letras maiúsculas em minúsculas e inserimos espaços entre palavras e sinais de pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nmt(text):\n",
    "    \"\"\"Pré-processa o conjunto de dados Inglês-Francês.\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Substitui espaços não-quebráveis por espaços normais e converte letras maiúsculas\n",
    "    # para minúsculas\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insere espaço entre palavras e sinais de pontuação\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447b223",
   "metadata": {},
   "source": [
    "## 9.5.2. Tokenização\n",
    "\n",
    "Para tradução automática, preferimos tokenização em nível de palavra em vez de tokenização em nível de caractere. A função a seguir tokeniza os primeiros `num_examples` pares de sequências de texto, onde cada token é uma palavra ou um sinal de pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokeniza o conjunto de dados Inglês-Francês.\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "print(source[:6])\n",
    "print(target[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7009cd",
   "metadata": {},
   "source": [
    "Vamos criar um histograma do número de tokens por sequência de texto. Neste conjunto de dados simples de Inglês-Francês, a maioria das sequências de texto tem menos de 20 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o tamanho da figura\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Cria o histograma\n",
    "source_lengths = [len(l) for l in source]\n",
    "target_lengths = [len(l) for l in target]\n",
    "\n",
    "plt.hist([source_lengths, target_lengths], bins=20, label=['origem', 'destino'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Histograma de Comprimentos de Sequência')\n",
    "plt.xlabel('Comprimento')\n",
    "plt.ylabel('Contagem')\n",
    "plt.show()\n",
    "\n",
    "# Imprime algumas estatísticas\n",
    "print(f\"Comprimento máximo da origem: {max(source_lengths)}\")\n",
    "print(f\"Comprimento máximo do destino: {max(target_lengths)}\")\n",
    "print(f\"Comprimento médio da origem: {sum(source_lengths)/len(source_lengths):.2f}\")\n",
    "print(f\"Comprimento médio do destino: {sum(target_lengths)/len(target_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34312e9",
   "metadata": {},
   "source": [
    "## 9.5.3. Vocabulário\n",
    "\n",
    "Como nosso conjunto de dados de tradução automática consiste em pares de idiomas, precisamos construir dois vocabulários, um para o idioma de origem (Inglês) e outro para o idioma de destino (Francês). Com a tokenização em nível de palavra, o tamanho do vocabulário será significativamente maior do que usando tokenização em nível de caractere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulário para texto.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        \n",
    "        # Conta frequências de tokens\n",
    "        counter = Counter([token for line in tokens for token in line])\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Cria mapeamento token para índice\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        # Adiciona tokens que atendem ao limiar de frequência\n",
    "        for token, freq in self.token_freqs:\n",
    "            if freq >= min_freq and token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.token_to_idx['<unk>'])\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "# Cria vocabulário de origem\n",
    "src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "print(f\"Tamanho do vocabulário de origem: {len(src_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria vocabulário de destino\n",
    "tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "print(f\"Tamanho do vocabulário de destino: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936be33",
   "metadata": {},
   "source": [
    "## 9.5.4. Carregando o Conjunto de Dados\n",
    "\n",
    "Para eficiência computacional, processamos minilotes de sequências com o mesmo comprimento através de truncamento e preenchimento. Se uma sequência tiver menos que `num_steps` tokens, preenchemos com o token `<pad>`. Se tiver mais que `num_steps` tokens, truncamos para manter apenas os primeiros `num_steps` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Trunca ou preenche sequências.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Trunca\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Preenche\n",
    "\n",
    "# Exemplo de truncamento e preenchimento\n",
    "sample_line = src_vocab[source[0]]  # Converte tokens para índices\n",
    "padded_line = truncate_pad(sample_line, 10, src_vocab['<pad>'])\n",
    "print(f\"Linha original: {source[0]}\")\n",
    "print(f\"Linha indexada: {sample_line}\")\n",
    "print(f\"Linha preenchida (tamanho 10): {padded_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511d3bc",
   "metadata": {},
   "source": [
    "Agora definimos uma função para transformar sequências de texto em minilotes para treinamento. Adicionamos o token especial `<eos>` ao final de cada sequência para indicar o fim da sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transforma sequências de texto de tradução automática em minilotes.\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).sum(1)\n",
    "    return array, valid_len\n",
    "\n",
    "# Exemplo de construção de arrays\n",
    "src_array, src_valid_len = build_array_nmt(source[:3], src_vocab, 10)\n",
    "print(\"Array de origem:\")\n",
    "print(src_array)\n",
    "print(\"\\nComprimentos válidos:\")\n",
    "print(src_valid_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f942a",
   "metadata": {},
   "source": [
    "## 9.5.5. Juntando Tudo\n",
    "\n",
    "Finalmente, definimos a função `load_data_nmt` para retornar o iterador de dados junto com os vocabulários de origem e destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299673f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Retorna o iterador e os vocabulários do conjunto de dados de tradução.\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = Vocab(source, min_freq=2,\n",
    "                     reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2,\n",
    "                     reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    \n",
    "    # Cria o DataLoader do PyTorch\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    return data_iter, src_vocab, tgt_vocab\n",
    "\n",
    "# Cria um pequeno iterador de dados\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
    "\n",
    "# Examina o primeiro lote\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', X)\n",
    "    print('Comprimentos válidos para X:', X_valid_len)\n",
    "    print('Y:', Y)\n",
    "    print('Comprimentos válidos para Y:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb32e0",
   "metadata": {},
   "source": [
    "Vamos testar nosso carregamento de dados lendo o primeiro minilote do conjunto de dados Inglês-Francês:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63936802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe base do codificador\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"A interface base do codificador para a arquitetura codificador-decodificador.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Classe base do decodificador\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"A interface base do decodificador para a arquitetura codificador-decodificador.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Arquitetura Codificador-Decodificador\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"A classe base para a arquitetura codificador-decodificador.\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174e1d",
   "metadata": {},
   "source": [
    "## 9.5.6. Implementação do Modelo Sequência-para-Sequência\n",
    "\n",
    "Agora vamos implementar um modelo sequência-para-sequência (seq2seq) com uma arquitetura codificador-decodificador para nossa tarefa de tradução automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d66498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"O codificador RNN para aprendizagem sequência-para-sequência.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Camada de embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X formato: (batch_size, seq_len)\n",
    "        # Primeiro, converte X para formato: (seq_len, batch_size) para RNN\n",
    "        X = X.T\n",
    "        # Converte de índices de token para embeddings\n",
    "        X = self.embedding(X)  # formato: (seq_len, batch_size, embed_size)\n",
    "        # A saída `X` formato: (seq_len, batch_size, num_hiddens)\n",
    "        # `state` formato: (num_layers, batch_size, num_hiddens)\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` formato: (seq_len, batch_size, num_hiddens)\n",
    "        # `state` formato: (num_layers, batch_size, num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do codificador\n",
    "encoder = Seq2SeqEncoder(vocab_size=len(src_vocab), embed_size=8, num_hiddens=16,\n",
    "                      num_layers=2, dropout=0.1)\n",
    "batch_size, seq_len = 4, 7\n",
    "X = torch.ones((batch_size, seq_len), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "print(f\"Formato da saída do codificador: {output.shape}\")\n",
    "print(f\"Formato do estado do codificador: {state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71444e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    \"\"\"O decodificador RNN para aprendizagem sequência-para-sequência.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        # Usa o estado final do codificador como estado inicial do decodificador\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X formato: (batch_size, seq_len)\n",
    "        # Primeiro, converte X para formato: (seq_len, batch_size) para RNN\n",
    "        X = X.T\n",
    "        # Obtém o último estado escondido do estado do codificador\n",
    "        # Transmite o contexto para (seq_len, batch_size, num_hiddens)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        # Incorpora a entrada\n",
    "        X = self.embedding(X)  # (seq_len, batch_size, embed_size)\n",
    "        # Concatena o contexto e os embeddings\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        # Calcula as saídas do decodificador\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        # Aplica a camada linear final\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # `output` formato: (batch_size, seq_len, vocab_size)\n",
    "        # `state` formato: (num_layers, batch_size, num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af330",
   "metadata": {},
   "source": [
    "### Codificador RNN\n",
    "\n",
    "Agora vamos implementar um codificador RNN para aprendizagem sequência-para-sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do decodificador\n",
    "decoder = Seq2SeqDecoder(vocab_size=len(tgt_vocab), embed_size=8, num_hiddens=16,\n",
    "                      num_layers=2, dropout=0.1)\n",
    "state = encoder(X)[1]\n",
    "output, state = decoder(X, state)\n",
    "print(f\"Formato da saída do decodificador: {output.shape}\")\n",
    "print(f\"Formato do estado do decodificador: {state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45029b90",
   "metadata": {},
   "source": [
    "Vamos testar o codificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mascara entradas irrelevantes em sequências.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                      device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"A perda de entropia cruzada softmax com máscaras.\"\"\"\n",
    "    # `pred` formato: (batch_size, seq_len, vocab_size)\n",
    "    # `label` formato: (batch_size, seq_len)\n",
    "    # `valid_len` formato: (batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a2b8a",
   "metadata": {},
   "source": [
    "### Decodificador RNN\n",
    "\n",
    "Agora vamos implementar um decodificador RNN para aprendizagem sequência-para-sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de recorte de gradiente\n",
    "def grad_clipping(model, theta):\n",
    "    \"\"\"Recorta o gradiente.\"\"\"\n",
    "    if isinstance(model, nn.Module):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = model.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "# Função de treinamento\n",
    "def train_seq2seq(model, data_iter, lr, num_epochs, device):\n",
    "    \"\"\"Treina um modelo seq2seq.\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    \n",
    "    model.apply(xavier_init_weights)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = MaskedSoftmaxCELoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                              device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat, _ = model(X, dec_input)\n",
    "            loss = loss_fn(Y_hat, Y, Y_valid_len)\n",
    "            loss.sum().backward()  # Torna a perda escalar para backward()\n",
    "            grad_clipping(model, 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.sum().item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'época {epoch + 1}, perda {avg_loss:.3f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f902e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros do modelo\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs = 0.005, 50\n",
    "\n",
    "# Define o dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cria o iterador de dados e o vocabulário\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
    "\n",
    "# Cria o codificador, decodificador e o modelo completo\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "# Treina o modelo\n",
    "train_seq2seq(model, train_iter, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4f7f6",
   "metadata": {},
   "source": [
    "### Treinando o Modelo\n",
    "\n",
    "Agora vamos definir a função de perda e o procedimento de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b67250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
    "    \"\"\"Prevê para sequência para sequência.\"\"\"\n",
    "    # Define o modelo no modo de avaliação\n",
    "    model.eval()\n",
    "    \n",
    "    # Processa a frase de entrada\n",
    "    src_tokens = src_sentence.lower().split(' ')\n",
    "    src_tokens = ['<bos>'] + src_tokens + ['<eos>']\n",
    "    \n",
    "    # Converte tokens para índices\n",
    "    src_indices = [src_vocab[token] for token in src_tokens]\n",
    "    \n",
    "    # Preenche até o tamanho requerido\n",
    "    if len(src_indices) < num_steps:\n",
    "        src_indices += [src_vocab['<pad>']] * (num_steps - len(src_indices))\n",
    "    else:\n",
    "        src_indices = src_indices[:num_steps]\n",
    "    \n",
    "    # Converte para tensor e adiciona dimensão de lote\n",
    "    enc_X = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Obtém saídas do codificador e inicializa o estado do decodificador\n",
    "    enc_outputs = model.encoder(enc_X)\n",
    "    dec_state = model.decoder.init_state(enc_outputs)\n",
    "    \n",
    "    # Inicializa a entrada do decodificador com token <bos>\n",
    "    dec_X = torch.tensor([[tgt_vocab['<bos>']]], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Gera tradução\n",
    "    output_tokens = []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # Obtém o token com a maior previsão\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred_token = tgt_vocab.idx_to_token[dec_X.squeeze(0).item()]\n",
    "        \n",
    "        # Para se previmos <eos> ou <pad>\n",
    "        if pred_token in ['<eos>', '<pad>']:\n",
    "            break\n",
    "        output_tokens.append(pred_token)\n",
    "        \n",
    "    return ' '.join(output_tokens)\n",
    "\n",
    "def translate(model, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
    "    \"\"\"Traduz uma frase da origem para o destino.\"\"\"\n",
    "    translation = predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab,\n",
    "                               num_steps, device)\n",
    "    print(f'Origem: {src_sentence}')\n",
    "    print(f'Tradução: {translation}')\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a1487",
   "metadata": {},
   "source": [
    "Agora vamos definir a função de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frases em inglês de exemplo para traduzir\n",
    "english_sentences = [\n",
    "    'go .',\n",
    "    'i am hungry .',\n",
    "    'he is running .'\n",
    "]\n",
    "\n",
    "# Traduz cada frase\n",
    "for sentence in english_sentences:\n",
    "    translate(model, sentence, src_vocab, tgt_vocab, num_steps, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515c6c1",
   "metadata": {},
   "source": [
    "### Criando e Treinando o Modelo Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1583cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs = 0.005, 300\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                         dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                         dropout)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(model, train_iter, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348f5fa",
   "metadata": {},
   "source": [
    "### Previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"Prevê para sequência para sequência.\"\"\"\n",
    "    # Define o modelo no modo de avaliação para inferência\n",
    "    model.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
    "    src_len = len(src_tokens)\n",
    "    if src_len < num_steps:\n",
    "        src_tokens += [src_vocab['<pad>']] * (num_steps - src_len)\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = model.encoder(enc_X)\n",
    "    dec_state = model.decoder.init_state(enc_outputs)\n",
    "    # Adiciona a dimensão do lote\n",
    "    dec_X = torch.unsqueeze(\n",
    "        torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device),\n",
    "        dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # Usamos o token com a maior probabilidade de previsão como entrada\n",
    "        # do decodificador no próximo passo de tempo\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Uma vez que o token de fim de sequência é previsto, a geração para\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff79101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, src_vocab, tgt_vocab, num_steps, device):\n",
    "    \"\"\"Traduz uma frase da origem para o destino.\"\"\"\n",
    "    translation = predict_seq2seq(model, src_sentence, src_vocab, tgt_vocab,\n",
    "                                  num_steps, device)\n",
    "    print(f'Origem: {src_sentence}')\n",
    "    print(f'Tradução: {translation}')\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a70c3",
   "metadata": {},
   "source": [
    "Vamos tentar traduzir algumas frases de exemplo do inglês para o francês:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f064a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frases de exemplo em inglês\n",
    "english_sentences = [\n",
    "    'go .',\n",
    "    'i am hungry .',\n",
    "    'he is running .'\n",
    "]\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    translate(model, sentence, src_vocab, tgt_vocab, num_steps, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354ccbc",
   "metadata": {},
   "source": [
    "# Discussão\n",
    "\n",
    "## Análise dos Resultados da Tradução (Questão 1 da Seção 9.5.7)\n",
    "\n",
    "Com base em nossos experimentos com o modelo de tradução automática, podemos fazer várias observações sobre os resultados da tradução:\n",
    "\n",
    "1. **Desempenho Básico da Tradução**: O modelo consegue aprender traduções básicas para frases simples. Para frases curtas com vocabulário comum como \"go .\" ou \"i am hungry .\", as traduções são geralmente razoáveis.\n",
    "\n",
    "2. **Limitações de Vocabulário**: Como filtramos palavras que aparecem menos de duas vezes nos dados de treinamento, o vocabulário é limitado. Isso significa que palavras raras provavelmente serão tratadas como tokens desconhecidos, levando à perda de informações na tradução.\n",
    "\n",
    "3. **Gramática e Contexto**: O modelo às vezes tem dificuldades com precisão gramatical, particularmente com concordância de gênero e conjugações verbais em francês, que dependem do contexto que a arquitetura simples de codificador-decodificador pode não capturar completamente.\n",
    "\n",
    "4. **Impacto do Tamanho da Sequência**: O desempenho diminui com frases mais longas, pois o modelo tem dificuldade em manter o contexto em sequências mais longas. Esta é uma limitação conhecida dos modelos básicos de sequência-para-sequência sem mecanismos de atenção.\n",
    "\n",
    "5. **Efeito do Tamanho dos Dados**: Usamos apenas um pequeno subconjunto do conjunto de dados Tatoeba (600 exemplos), o que limita a capacidade do modelo de generalizar para uma ampla variedade de frases.\n",
    "\n",
    "## Por que Usar a Arquitetura Sequência-para-Sequência para Tradução Automática (Questão 2 da Seção 9.5.7)\n",
    "\n",
    "As arquiteturas de sequência-para-sequência (seq2seq) são particularmente adequadas para tarefas de tradução automática por várias razões importantes:\n",
    "\n",
    "1. **Tratamento de Comprimento Variável**: A tradução automática requer mapeamento entre sequências de diferentes comprimentos - frases de origem e suas traduções raramente têm o mesmo número de palavras. Os modelos seq2seq lidam naturalmente com essa exigência de entrada e saída de comprimento variável.\n",
    "\n",
    "2. **Preservação de Dependências Sequenciais**: Ambos os idiomas têm dependências sequenciais onde a ordem das palavras e o contexto importam. A arquitetura codificador-decodificador captura essas dependências tanto no idioma de origem quanto no de destino.\n",
    "\n",
    "3. **Preservação de Contexto**: O codificador comprime toda a frase de origem em um vetor de contexto (ou uma série de estados ocultos) que encapsula o significado da sequência de entrada. Isso permite que o decodificador gere uma tradução que considera o significado de toda a frase de origem.\n",
    "\n",
    "4. **Aprendizado Fim-a-Fim**: Os modelos seq2seq aprendem o mapeamento de tradução diretamente de corpora paralelos sem exigir regras linguísticas explícitas, o que é valioso dada a complexidade da tradução de idiomas.\n",
    "\n",
    "5. **Flexibilidade Arquitetônica**: A estrutura seq2seq permite vários aprimoramentos, como mecanismos de atenção, que ajudam a resolver o gargalo de informações no vetor de contexto e melhoram significativamente a qualidade da tradução para frases mais longas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
